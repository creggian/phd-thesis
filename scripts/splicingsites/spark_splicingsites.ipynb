{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preload of a personal library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.addPyFile(\"https://raw.githubusercontent.com/creggian/creggian-python/master/dist/creggian.zip\")\n",
    "from creggian.main import coords2bin, to_tsv_line\n",
    "from creggian.bed import leftjoin_overlap, overlaps_any\n",
    "from creggian.parse import split_tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Download BAM file from encode, eg: https://www.encodeproject.org/files/ENCFF753KWK/@@download/ENCFF753KWK.bam\n",
    "- Calculate the coverage BDG file using bedtools 'bam2bdg.sh' (bedtools genomecov -bga -split ...). '-bga' option will fill non-covered regions with 0 values, hence the entire chromosome is represented in the resulting files. Chromosome border coverage are special cases that we can neglect for the purpose of this project. Output format: chr, start, end, value. 0-based coordinate system.\n",
    "- Calculate the delta coverages using bedtools 'bdg2deltacoverage.sh' (bedtools closest -io -iu ...). Output format: chr, pos_0based, delta, distance. If 'pos_0based' is 10001, then its correspondence to 'pos_1based' is 10001-10002. Value 'delta' corresponds to prev_region - next_region (left - right, coordinate-wise). Value 'distance' is indeed the distance between the two regions, it has always to be 1, otherwise there is an error.\n",
    "- Move everything to HDFS, that means: bam, bdg, bdg.dc\n",
    "- Use ADAM framework to transform bam, bdg, bdg.dc files into parquet files ('bam2adam.sh', 'bdg2parquet.sh', 'bdgdc2parquet.sh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ENCFF015DEA.bdg.dc.parquet',\n",
       " 'ENCFF041THY.bdg.dc.parquet',\n",
       " 'ENCFF113PDT.bdg.dc.parquet',\n",
       " 'ENCFF220QDT.bdg.dc.parquet',\n",
       " 'ENCFF226SQK.bdg.dc.parquet',\n",
       " 'ENCFF229XLD.bdg.dc.parquet',\n",
       " 'ENCFF243QVH.bdg.dc.parquet',\n",
       " 'ENCFF286ZEF.bdg.dc.parquet',\n",
       " 'ENCFF331ZDV.bdg.dc.parquet',\n",
       " 'ENCFF391XTO.bdg.dc.parquet',\n",
       " 'ENCFF397QLJ.bdg.dc.parquet',\n",
       " 'ENCFF434OIG.bdg.dc.parquet',\n",
       " 'ENCFF513CAU.bdg.dc.parquet',\n",
       " 'ENCFF602BYA.bdg.dc.parquet',\n",
       " 'ENCFF633PEY.bdg.dc.parquet',\n",
       " 'ENCFF677URC.bdg.dc.parquet',\n",
       " 'ENCFF741AGL.bdg.dc.parquet',\n",
       " 'ENCFF746HOG.bdg.dc.parquet',\n",
       " 'ENCFF753KWK.bdg.dc.parquet',\n",
       " 'ENCFF803AAV.bdg.dc.parquet',\n",
       " 'ENCFF892QFE.bdg.dc.parquet',\n",
       " 'ENCFF907ILS.bdg.dc.parquet',\n",
       " 'ENCFF927WKN.bdg.dc.parquet',\n",
       " 'ENCFF954GHY.bdg.dc.parquet']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hdfs_ls(basepath, regex='.*'):\n",
    "    import re\n",
    "    \n",
    "    hdfsResult = !hdfs dfs -ls {basepath}\n",
    "    hdfsResultList = hdfsResult.list\n",
    "\n",
    "    fullpaths = []\n",
    "    for x in hdfsResultList:\n",
    "        pos = x.find(basepath)\n",
    "        if (pos >= 0):\n",
    "            fullpaths += [x[pos:len(x)]]\n",
    "\n",
    "    filenames = []\n",
    "    for x in fullpaths:\n",
    "        fname = x.split(\"/\")[-1]\n",
    "        if re.search(regex, fname):\n",
    "            filenames += [fname]\n",
    "\n",
    "    return filenames\n",
    "\n",
    "basepath = '/user/creggian/ENCODE/hg19/brain/fetal/'\n",
    "parquets = hdfs_ls(basepath, regex='bdg.dc.parquet')\n",
    "parquets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+-----+--------+\n",
      "|       chr|pos|delta|distance|\n",
      "+----------+---+-----+--------+\n",
      "|ERCC-00002|  1|   -5|       1|\n",
      "|ERCC-00002|  2|  -24|       1|\n",
      "|ERCC-00002|  3|  -19|       1|\n",
      "|ERCC-00002|  4|   -5|       1|\n",
      "|ERCC-00002|  7|   -3|       1|\n",
      "|ERCC-00002|  9|  -21|       1|\n",
      "|ERCC-00002| 10|  -12|       1|\n",
      "|ERCC-00002| 11|   -5|       1|\n",
      "|ERCC-00002| 12|   -2|       1|\n",
      "|ERCC-00002| 13|   -6|       1|\n",
      "|ERCC-00002| 14|   -4|       1|\n",
      "|ERCC-00002| 15|   -5|       1|\n",
      "|ERCC-00002| 16|   -6|       1|\n",
      "|ERCC-00002| 18|   -3|       1|\n",
      "|ERCC-00002| 19|   -8|       1|\n",
      "|ERCC-00002| 20|  -72|       1|\n",
      "|ERCC-00002| 21|  -57|       1|\n",
      "|ERCC-00002| 22| -111|       1|\n",
      "|ERCC-00002| 23| -145|       1|\n",
      "|ERCC-00002| 24|  -99|       1|\n",
      "+----------+---+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bdg_ss_dict = {}\n",
    "for filename in parquets:\n",
    "    data = spark.read.parquet(basepath + filename)\n",
    "    bdg_ss_dict[filename] = data\n",
    "    \n",
    "bdg_ss_dict[\"ENCFF015DEA.bdg.dc.parquet\"].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bdg_ss_view_appendix = \"_bdg_ss\"\n",
    "for filename in parquets:\n",
    "    bam_filename = filename.split(\".\")[0]  # ENCFF513CAU.bdg.parquet => ENCFF513CAU\n",
    "    bdg_ss_dict[filename].createOrReplaceTempView(bam_filename + bdg_ss_view_appendix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter according to the delta coverage and distance. Create the auxiliary column to group by: \"chr_pos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+--------+----------+------------+------------+\n",
      "| chr|  pos|delta|distance|   chr_pos|is_neg_delta|is_pos_delta|\n",
      "+----+-----+-----+--------+----------+------------+------------+\n",
      "|chr1|10000|  -29|       1|chr1_10000|           1|           0|\n",
      "|chr1|10001|  -79|       1|chr1_10001|           1|           0|\n",
      "|chr1|10002|  -22|       1|chr1_10002|           1|           0|\n",
      "|chr1|10003| -124|       1|chr1_10003|           1|           0|\n",
      "|chr1|10004| -179|       1|chr1_10004|           1|           0|\n",
      "|chr1|10005|  -79|       1|chr1_10005|           1|           0|\n",
      "|chr1|10006|  -26|       1|chr1_10006|           1|           0|\n",
      "|chr1|10007|  -85|       1|chr1_10007|           1|           0|\n",
      "|chr1|10009|  -24|       1|chr1_10009|           1|           0|\n",
      "|chr1|10013|  -24|       1|chr1_10013|           1|           0|\n",
      "|chr1|10016|  -44|       1|chr1_10016|           1|           0|\n",
      "|chr1|10034|  -21|       1|chr1_10034|           1|           0|\n",
      "|chr1|10077|   27|       1|chr1_10077|           0|           1|\n",
      "|chr1|10083|   32|       1|chr1_10083|           0|           1|\n",
      "|chr1|10089|   27|       1|chr1_10089|           0|           1|\n",
      "|chr1|10095|   26|       1|chr1_10095|           0|           1|\n",
      "|chr1|10099|   26|       1|chr1_10099|           0|           1|\n",
      "|chr1|10100|   22|       1|chr1_10100|           0|           1|\n",
      "|chr1|10101|   66|       1|chr1_10101|           0|           1|\n",
      "|chr1|10102|   26|       1|chr1_10102|           0|           1|\n",
      "+----+-----+-----+--------+----------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "def df_ss_filtering(fview, dc=0, dist=1):\n",
    "    query  = \"SELECT * FROM \" + fview\n",
    "    query += \" WHERE abs(delta) >= \" + str(dc)\n",
    "    query += \" AND distance == \" + str(dist)\n",
    "    \n",
    "    return spark.sql(query)\n",
    "\n",
    "\n",
    "def concat_chr_pos(chrom, pos):\n",
    "    return str(chrom) + \"_\" + str(pos)\n",
    "\n",
    "\n",
    "def is_neg_delta(delta):\n",
    "    ret = 0\n",
    "    if delta < 0:\n",
    "        ret = 1\n",
    "    return ret\n",
    "\n",
    "\n",
    "def is_pos_delta(delta):\n",
    "    ret = 0\n",
    "    if delta > 0:\n",
    "        ret = 1\n",
    "    return ret\n",
    "\n",
    "\n",
    "udf_concat_chr_pos = udf(concat_chr_pos, StringType())\n",
    "udf_is_neg_delta = udf(is_neg_delta, LongType())\n",
    "udf_is_pos_delta = udf(is_pos_delta, LongType())\n",
    "\n",
    "bdg_ss_dict_targets = {}\n",
    "for filename in parquets:\n",
    "    fview = filename.split(\".\")[0] + bdg_ss_view_appendix\n",
    "    bdg_ss_dict_target = df_ss_filtering(fview, dc=20, dist=1)\n",
    "    bdg_ss_dict_target = bdg_ss_dict_target.withColumn(\"chr_pos\", udf_concat_chr_pos(\"chr\", \"pos\"))\n",
    "    bdg_ss_dict_target = bdg_ss_dict_target.withColumn(\"is_neg_delta\", udf_is_neg_delta(\"delta\"))\n",
    "    bdg_ss_dict_target = bdg_ss_dict_target.withColumn(\"is_pos_delta\", udf_is_pos_delta(\"delta\"))\n",
    "    bdg_ss_dict_targets[filename] = bdg_ss_dict_target\n",
    "\n",
    "bdg_ss_dict_targets[\"ENCFF041THY.bdg.dc.parquet\"].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Union all dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def union_all(dfs):\n",
    "    if len(dfs) > 1:\n",
    "        return dfs[0].unionAll(union_all(dfs[1:]))\n",
    "    else:\n",
    "        return dfs[0]\n",
    "    \n",
    "dfs = union_all(bdg_ss_dict_targets.values())\n",
    "#dfs.count()  # 15641227"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+-----+--------+--------------+------------+------------+\n",
      "|       chr|pos|delta|distance|       chr_pos|is_neg_delta|is_pos_delta|\n",
      "+----------+---+-----+--------+--------------+------------+------------+\n",
      "|ERCC-00002| 22|  -23|       1| ERCC-00002_22|           1|           0|\n",
      "|ERCC-00002| 25|  -37|       1| ERCC-00002_25|           1|           0|\n",
      "|ERCC-00002| 26|  -29|       1| ERCC-00002_26|           1|           0|\n",
      "|ERCC-00002| 64|  -31|       1| ERCC-00002_64|           1|           0|\n",
      "|ERCC-00002| 68|  -20|       1| ERCC-00002_68|           1|           0|\n",
      "|ERCC-00002| 71|  -37|       1| ERCC-00002_71|           1|           0|\n",
      "|ERCC-00002| 91|  -21|       1| ERCC-00002_91|           1|           0|\n",
      "|ERCC-00002| 93|  -21|       1| ERCC-00002_93|           1|           0|\n",
      "|ERCC-00002|101|  -22|       1|ERCC-00002_101|           1|           0|\n",
      "|ERCC-00002|123|   21|       1|ERCC-00002_123|           0|           1|\n",
      "|ERCC-00002|127|   22|       1|ERCC-00002_127|           0|           1|\n",
      "|ERCC-00002|140|  -34|       1|ERCC-00002_140|           1|           0|\n",
      "|ERCC-00002|163|  -26|       1|ERCC-00002_163|           1|           0|\n",
      "|ERCC-00002|172|   23|       1|ERCC-00002_172|           0|           1|\n",
      "|ERCC-00002|174|  -23|       1|ERCC-00002_174|           1|           0|\n",
      "|ERCC-00002|179|  -28|       1|ERCC-00002_179|           1|           0|\n",
      "|ERCC-00002|181|  -24|       1|ERCC-00002_181|           1|           0|\n",
      "|ERCC-00002|183|  -24|       1|ERCC-00002_183|           1|           0|\n",
      "|ERCC-00002|187|  -23|       1|ERCC-00002_187|           1|           0|\n",
      "|ERCC-00002|200|  -21|       1|ERCC-00002_200|           1|           0|\n",
      "+----------+---+-----+--------+--------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group by \"chr_pos\" and filter for presence of splicing size greater than 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "\n",
    "\n",
    "def split_chr_pos_chr(chr_pos):\n",
    "    return str(chr_pos).split(\"_\")[0]\n",
    "\n",
    "\n",
    "def split_chr_pos_pos(chr_pos):\n",
    "    return int(float(str(chr_pos).split(\"_\")[1]))\n",
    "\n",
    "\n",
    "def reduce_neg_pos_delta(x, y):\n",
    "    if x[0] != y[0]:\n",
    "        raise Exception(\"In 'reduce_neg_pos_delta' reduced 'pos' are not the same !\")\n",
    "    return (x[0], x[1] + y[1], x[2] + y[2])\n",
    "\n",
    "\n",
    "def sort_rbk(x):\n",
    "    chr_pos, is_delta = x\n",
    "    poss, is_neg_delta, is_pos_delta = is_delta\n",
    "    \n",
    "    strand = \"+\"\n",
    "    n = is_pos_delta\n",
    "    if (int(is_neg_delta) > int(is_pos_delta)):\n",
    "        strand = \"-\"\n",
    "        n = is_neg_delta\n",
    "    \n",
    "    chrom = split_chr_pos_chr(chr_pos)\n",
    "    pos = split_chr_pos_pos(chr_pos)\n",
    "    return (chrom, int(pos), strand, int(n))\n",
    "\n",
    "\n",
    "def filter_threshold(x, th):\n",
    "    n = int(x[3])\n",
    "    return n >= th\n",
    "\n",
    "\n",
    "# reduceByKey => (chr1_10006, (10006, 1, 0))\n",
    "th = len(bdg_ss_dict_targets)*threshold\n",
    "dfs_threshold = dfs.rdd\\\n",
    "    .map(lambda x: (x[4], (x[1], x[5], x[6])))\\\n",
    "    .reduceByKey(reduce_neg_pos_delta)\\\n",
    "    .map(sort_rbk)\\\n",
    "    .filter(lambda x: filter_threshold(x, th))\\\n",
    "    .map(to_tsv_line)\\\n",
    "    .saveAsTextFile(\"/user/creggian/datarepo/fetalBrain_ss_dc20_freq05_20170223.bed\")\n",
    "    \n",
    "#dfs_threshold.first()  # 434043"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#threshold = 0.5\n",
    "#\n",
    "#dfs_threshold = dfs.groupBy(\"chr_pos\")\\\n",
    "#    .count()\\\n",
    "#    .withColumnRenamed(\"count\", \"n\")\\\n",
    "#    .filter(\"n >= \" + str(len(bdg_ss_dict_targets)*threshold))\\\n",
    "#    .withColumn(\"chr\", udf_split_chr_pos_chr(\"chr_pos\"))\\\n",
    "#    .withColumn(\"pos\", udf_split_chr_pos_pos(\"chr_pos\"))\n",
    "#    \n",
    "#dfs_threshold.count()  # 434043"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
